{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables,train_corpus,min_count=1\n",
    "def build_vocab(model, sentences, min_count, trim_rule=None, null_word=False):\n",
    "    vocabs = scan_vocab(model, sentences, trim_rule=trim_rule)\n",
    "    scale_vocab(model, vocabs, min_count)\n",
    "    finalize_vocab(model, null_word)\n",
    "'''\n",
    "统计document的词频：vocab{词：词频}(后面需要根据词频过滤一些词);\n",
    "              标签词长：docvecs{tag：索引，tag对应句子的词总个数}\n",
    "'''\n",
    "def scan_vocab(model, documents, trim_rule=None):\n",
    "    document_no = -1\n",
    "    min_reduce = 1\n",
    "    vocab = defaultdict(int)\n",
    "    new_tags = []\n",
    "    for document_no, document in enumerate(documents):\n",
    "        document_length = len(document.words)\n",
    "\n",
    "        for tag in document.tags:\n",
    "            if tag not in model.docvecs:\n",
    "                new_tags.append(tag)\n",
    "                # docvecs  (TAG,(INDEX,SUM(SEN_LEN) ))\n",
    "                model.docvecs[tag] = [len(model.offset2doctag), document_length]\n",
    "                # NI  docvecs\n",
    "                model.offset2doctag.append(tag)\n",
    "            else:\n",
    "                model.docvecs[tag][1] += document_length\n",
    "        for word in document.words:\n",
    "            # 词频\n",
    "            vocab[word] += 1\n",
    "        # 内 存 \n",
    "        if model.max_vocab_size and len(vocab) > model.max_vocab_size:\n",
    "            utils.prune_vocab(vocab, min_reduce, trim_rule=trim_rule)\n",
    "\n",
    "    model.n_newTags = len(new_tags)\n",
    "    model.corpus_count = document_no + 1\n",
    "    return vocab\n",
    "'''\n",
    "将scan_vocab的词频按过滤规则加入model.vocab，model.vocab = {} key: word(str) -> [wordindex, count, Intger.MAX_VALUE*word_probability]\n",
    "并计算新词的 word_probability（可能被丢弃的概率）,即论文中对高频词的处理\n",
    "'''\n",
    "def scale_vocab(model, vocabs, min_count, dry_run=False):\n",
    "    # [v.index, v.count, v.sample_int]\n",
    "    sample = model.sample\n",
    "    drop_unique, drop_total = 0, 0\n",
    "    new_total, pre_exist_total = 0, 0\n",
    "    new_words, pre_exist_words = [], []\n",
    "    # model.vocab = {} key: word(str) -> [wordindex, count, sample_int]\n",
    "    n_words_exit = len(model.vocab)\n",
    "    for word, v in vocabs.items():\n",
    "        if v >= min_count:\n",
    "            if word in model.vocab:\n",
    "                pre_exist_words.append(word)\n",
    "                pre_exist_total += v\n",
    "                if not dry_run:\n",
    "                    model.vocab[word][1] += v\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "                new_total += v\n",
    "                if not dry_run:\n",
    "                    model.vocab[word] = [len(model.index2word), v, 0]\n",
    "                    model.index2word.append(word)\n",
    "        else:\n",
    "            drop_unique += 1\n",
    "            drop_total += v\n",
    "    n_newWords = len(new_words)\n",
    "    assert n_words_exit + n_newWords == len(model.vocab)\n",
    "    model.n_newWords = n_newWords\n",
    "\n",
    "    retain_words = new_words + pre_exist_words\n",
    "    retain_total = new_total + pre_exist_total\n",
    "    # Precalculate each vocabulary item's threshold for sampling\n",
    "    if not sample:\n",
    "        # no words downsampled\n",
    "        threshold_count = retain_total\n",
    "    elif sample < 1.0:\n",
    "        # traditional meaning: set parameter as proportion of total\n",
    "        threshold_count = sample * retain_total\n",
    "    else:\n",
    "        # new shorthand: sample >= 1 means downsample all words with higher count than sample\n",
    "        threshold_count = int(sample * (3 + sqrt(5)) / 2)\n",
    "\n",
    "    downsample_total, downsample_unique = 0, 0\n",
    "    for w in retain_words:\n",
    "        v = vocabs[w]\n",
    "        # P= 1- sqrt(threshold/f) ==> P = sqrt((f/threshold)+1)*(thresholf/f)) 防止0的出现\n",
    "        word_probability = (sqrt(v / threshold_count) + 1) * (threshold_count / v)\n",
    "        if word_probability < 1.0:\n",
    "            downsample_unique += 1\n",
    "            downsample_total += word_probability * v\n",
    "        else:\n",
    "            word_probability = 1.0\n",
    "            downsample_total += v\n",
    "        if not dry_run:\n",
    "            model.vocab[w][2] = int(round(word_probability * 2 ** 32))\n",
    "'''\n",
    "预先计算负采样的数组：model.cum_table是一个升序数组，最后一位是Intger.MAX_VALUE，\n",
    "是为让所有的词根据频率在model.cum_table上获得长度：len(w) = count(w) ** 0.75 /  E (count(u)) ** 0.75\n",
    "'''\n",
    "def finalize_vocab(model, null_word=False):\n",
    "    \"\"\"Build tables and model weights based on final vocabulary settings.\"\"\"\n",
    "    power = 0.75\n",
    "    domain = 2 ** 31 - 1\n",
    "    if model.negative:\n",
    "        # build the table for drawing random words (for negative sampling)\n",
    "        vocab_size = len(model.index2word)\n",
    "        model.cum_table = np.zeros(vocab_size)\n",
    "        # compute sum of all power (Z in paper)\n",
    "        train_words_pow = 0.0\n",
    "        for word_index in range(vocab_size):\n",
    "            train_words_pow += model.vocab[model.index2word[word_index]][1] ** power\n",
    "        cumulative = 0.0\n",
    "        # len(w) = count(w) ** 0.75 /  E (count(u)) ** 0.75\n",
    "        for word_index in range(vocab_size):\n",
    "            cumulative += model.vocab[model.index2word[word_index]][1] ** power\n",
    "            model.cum_table[word_index] = round(cumulative / train_words_pow * domain)\n",
    "            \n",
    "        if len(model.cum_table) > 0:\n",
    "            assert model.cum_table[-1] == domain\n",
    "            \n",
    "    if null_word and not '\\0' in model.vocab:\n",
    "        # create null pseudo-word for padding when using concatenative L1 (run-of-words)\n",
    "        # this word is only ever input – never predicted – so count, huffman-point, etc doesn't matter\n",
    "        word, v = '\\0', [len(model.vocab), 1, 0]\n",
    "        model.index2word.append(word)\n",
    "        model.vocab[word] = v\n",
    "'''\n",
    "采样函数\n",
    "'''\n",
    "def _random_sample_negative(model, predictword):\n",
    "#     采样一定会采到中心词\n",
    "    word_indices = [predictword[0]]\n",
    "    while len(word_indices) < model.negative + 1:\n",
    "#         model.cum_table[-1])==int的上限\n",
    "        w = model.cum_table.searchsorted(np.random.randint(model.cum_table[-1]))\n",
    "        # w=np.random.randint(len(model.cum_table))\n",
    "        if w != predictword[0]:\n",
    "            word_indices.append(w)\n",
    "    return word_indices\n",
    "''''''\n",
    "def generate_label(model, sentences):\n",
    "    batch, label = [], []\n",
    "    for sentence in sentences:\n",
    "        doctag_indexes = [model.docvecs[index][0] for index in sentence.tags if index in model.docvecs]\n",
    "\n",
    "        for word in sentence.words:\n",
    "            predict_word = model.vocab[word]\n",
    "            word_indices = _random_sample_negative(model,predict_word)\n",
    "            batch.extend(doctag_indexes)\n",
    "            for i in range(len(doctag_indexes)):\n",
    "                label.append(word_indices)\n",
    "                # batch ->tag  index\n",
    "    return np.array(batch), np.array(label)\n",
    "\n",
    "def generate_batch_words(model, sentences):\n",
    "    batch, label = [], []\n",
    "    for sentence in sentences:\n",
    "        word_vocabs = [model.vocab[w] for w in sentence.words if model.vocab[w][2] > np.random.rand() * 2 ** 32]\n",
    "        for pos, word in enumerate(word_vocabs):\n",
    "            # go over the words in window\n",
    "            start = max(0, pos - model.window)\n",
    "            for pos2, word2 in enumerate(word_vocabs[start:(pos + model.window + 1)], start):\n",
    "                # don't train on the `word` itmodel\n",
    "                if pos2 != pos:\n",
    "                    batch.append(word[0])\n",
    "                    word_index = model._random_sample_negative(word2)\n",
    "                    label.append(word_index)\n",
    "\n",
    "    return np.array(batch), np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfDoc2vec(object):\n",
    "    def __init__(self, negative, window, batch_words, other_model):\n",
    "        self.vocab = other_model.wv.vocab\n",
    "        self.index2word=other_model.wv.index2word\n",
    "        self.syn1neg = other_model.syn1neg.astype(np.float32)\n",
    "        self.syn0 = other_model.wv.syn0.astype(np.float32)\n",
    "        self.cum_table = other_model.cum_table\n",
    "        self.negative = negative\n",
    "        self.window = window\n",
    "        self.batch_words = batch_words\n",
    "        self.max_vocab_size = 10000000\n",
    "        self.corpus_count = 0\n",
    "        self.sample = other_model.sample\n",
    "        self.vector_size = 300\n",
    "        self.layer1_size = 300\n",
    "        self.docvecs = {}\n",
    "        self.offset2doctag = []\n",
    "        self.docvec_syn0 = []\n",
    "        self.n_newWords = None\n",
    "        self.n_newTags = None\n",
    "        self.learn_words = False\n",
    "        self.convertDataStructure()\n",
    "    #!!!\n",
    "    def train(self,sentences,epochs=None):\n",
    "        #init word2vec placeholder\n",
    "        #new word count\n",
    "        n_newWords = tf.placeholder(dtype=tf.int32, name='n_newWords')\n",
    "        #new tag count\n",
    "        n_newTags = tf.placeholder(dtype=tf.int32, name='n_newTags')\n",
    "        #单词plh\n",
    "        wv_plh = tf.placeholder(dtype=tf.float32, name='wv_plh')\n",
    "        #句向量plh\n",
    "        doc_vec = tf.placeholder(dtype=tf.float32, name='doc_vec')\n",
    "        #syn1neg\n",
    "        sp_plh = tf.placeholder(dtype=tf.float32, name='sp_plh')\n",
    "        \n",
    "        w2v_embeddings, new_vocabs = self.createWeights(wv_plh, n_newWords, self.vector_size, ['exist_vocabs', 'new_vocabs', 'w2v_embeddings'])\n",
    "        d2v_embeding, new_doc2vec = self.createWeights(doc_vec, n_newTags, self.layer1_size,  ['exist_doc2vec', 'new_doc2vec', 'd2v_embeding'])\n",
    "        nce_weights, new_nce_weights= self.createWeights(sp_plh, n_newWords, self.vector_size, ['exit_nce_weights', 'new_nce_weights', 'nce_weights'])\n",
    "\n",
    "        optimizer_sg, loss_sg = self.build_skip_gram(w2v_embeddings, nce_weights)\n",
    "        optimizer_db, loss_db= self.build_dbow(d2v_embeding, nce_weights)\n",
    "        \n",
    "        bacth_sentences, _ = get_batch_sentences(self,sentences)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            print('start train skip_gram model')\n",
    "            sess.run(tf.global_variables_initalizer(),\n",
    "                     feed_dict={\n",
    "                         n_newWords:self.n_newWords,\n",
    "                         n_newTags:self.n_newTags,\n",
    "                         wv_plh:self.syn0,\n",
    "                         doc_vec:[],\n",
    "                         sp_plh:self.syn1neg\n",
    "                     })\n",
    "            for i in range(epochs):\n",
    "                for sentences in bacth_sentences:\n",
    "                    # train word2vec skip_gram model\n",
    "                    if self.learn_words:\n",
    "                        inputs_sg,labels_sg = generate_batch_words(self, sentences)\n",
    "                        if len(labels_sg)>0 and len(inputs_sg)>0:\n",
    "                            feed_dict = {'inputs_sg:0': inputs_sg, 'labels_sg:0': labels_sg}\n",
    "                            _,cur_loss = sess.run([optimizer_sg, loss_sg],feed_dict=feed_dict)\n",
    "                    # train doc2vec distribution bags of word model\n",
    "                    inputs_db, labels_db = generate_label(self, sentences)\n",
    "                    feed_dict = {'inputs_db:0': inputs_db, 'labels_db:0': labels_db}\n",
    "                    _, cur_loss, doc_syn0, softmax_weight = sess.run(\n",
    "                        [optimizer_db,loss_db, d2v_embeding, nce_weights], \n",
    "                        feed_dict=feed_dict)\n",
    "                    print('loss:{} at epoch {}'.format(cur_loss,i))\n",
    "            self.docvec_syn0 = doc_syn0\n",
    "            self.syn1neg = softmax_weight\n",
    "    def convertDataStructure(self):\n",
    "        vocab = {word : [v.index, v.count, v.sample_int] for word, v in self.vocab.items()}\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    '''initial new_wv_syn0 , new_sp, new_doc_syn0'''\n",
    "    def init_weights(self,new_words,new_tags):\n",
    "        # set initial input/projection and hidden weights\n",
    "        newsyn0 = np.zeros((len(new_words),self.vector_size))\n",
    "        # randomize the remaining words\n",
    "        for i in range(len(newsyn0)):\n",
    "            # construct deterministic seed from word AND seed argument\n",
    "            newsyn0[i] = self.seeded_vector(new_words[i] + str(1))\n",
    "        if self.negative:\n",
    "            self.new_sp = np.zeros((len(new_words), self.layer1_size))\n",
    "        length=len(new_tags)\n",
    "        self.new_doc_syn0 = np.zeros((length, self.vector_size))\n",
    "        \n",
    "        for i in range(length):\n",
    "            # construct deterministic seed from index AND model seed\n",
    "            seed = \"%d %s\" % (model.seed, new_tags[i])\n",
    "            self.new_doc_syn0[i] = self.seeded_vector(seed)\n",
    "        self.new_wv_syn0 = newsyn0\n",
    "    \n",
    "    def seeded_vector(self, seed_string):\n",
    "        \"\"\"Create one 'random' vector (but deterministic by seed_string)\"\"\"\n",
    "        # Note: built-in hash() may vary by Python version or even (in Py3.x) per launch\n",
    "        once = np.random.RandomState(hash(seed_string) & 0xffffffff)\n",
    "        return (once.rand(self.vector_size) - 0.5) / self.vector_size\n",
    "    \n",
    "    def createWeights(self,exist,new,size,names):\n",
    "        exist_str, new_str, concat_str = names[0], names[1], names[2]\n",
    "        exist_weight=tf.Variable(exist,validate_shape=False,name=exist_str)\n",
    "        new_weight = tf.Variable(tf.truncated_normal([new,size],dyte=tf.float32),validate_shape=False,name=new_str)\n",
    "        embeddings = tf.case(\n",
    "            [(tf.equal(tf.shape(exist_weight)[0], 0), lambda: new_weight),\n",
    "            (tf.equal(tf.shape(new_weight)[0], 0), lambda: exist_weight)],\n",
    "            default=lambda: tf.concat([exist_weight, new_weight], axis=0))\n",
    "        #y=x的operation\n",
    "        embeddings = tf.identity(embeddings, name=concat_str)\n",
    "        return embeddings, new_weight\n",
    "    \n",
    "    def build_skip_gram(self, embeddings, nce_weights):\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[None], name='inputs_sg')\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[None, 1+self.negative], name='labels_sg')\n",
    "        \n",
    "        embed = tf.nn.embedding_lookup(embeddings,train_inputs)\n",
    "        layer_out = tf.nn.embedding_lookup(nce_weights,train_inputs)\n",
    "        \n",
    "        loss = self._train_sg_pair(embed,layer_out,train_inputs)\n",
    "        loss = tf.identity(loss,name='lose_sg')\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(loss,name='optimizer_sg')\n",
    "        return optimizer, loss\n",
    "    \n",
    "    def build_dbow(self,doc2vec, nce_weights):\n",
    "\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[None], name='inputs_db')\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[None, 1+self.negative], name='labels_db')\n",
    "\n",
    "        layer_in = tf.nn.embedding_lookup(doc2vec, train_inputs)\n",
    "        layer_out = tf.nn.embedding_lookup(nce_weights, train_labels)\n",
    "        #????\n",
    "        layer_out = tf.stop_gradient(layer_out)\n",
    "\n",
    "        loss = self._train_sg_pair(layer_in,layer_out,train_inputs)\n",
    "        loss = tf.identity(loss,name='lose_db')\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(loss,name='optimizer_db')\n",
    "        return optimizer, loss\n",
    "\n",
    "    # train_inputs 词的id   layer_in：输入词的向量   layer_out：输出的词和负例子                 \n",
    "    def _train_sg_pair(self,layer_in,layer_out,train_inputs):\n",
    "        layer_in=tf.expand_dims(layer_in,axis=1)\n",
    "        \n",
    "        #\n",
    "        logits =tf.reduce_sum(layer_in * layer_out, axis=2)\n",
    "        \n",
    "        labels_one_hot = tf.one_hot(tf.tile([0],multiples=tf.shape(train_inputs),self.negative+1))\n",
    "        \n",
    "        # label =》（1，0，0，0...）y_real 用来计算loss 将layer_in*layer_out 预测一个(0.8,0.02ds,...)的向量，计算loss\n",
    "        loss =tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=labels_one_hot,\n",
    "            logits=logits)\n",
    "        loss=tf.reduce_sum(loss,axis=1)\n",
    "        loss=tf.reduce_mean(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docSimilar(model, docs, doctag):\n",
    "\n",
    "    docMatrix = []\n",
    "    productNameVec = getVec(model, doctag)\n",
    "    productNameVec = np.array(productNameVec)\n",
    "    for row1 in productNameVec:   \n",
    "        docMatrixrow = []\n",
    "        for row2 in productNameVec:\n",
    "            temp = spatial.distance.cosine(row1, row2)\n",
    "            docMatrixrow.append(temp)\n",
    "        docMatrix.append(docMatrixrow)\n",
    "    docMatrix = np.array(docMatrix)\n",
    "    indexSku = docs.index\n",
    "    df_docSimilar = pd.DataFrame(docMatrix, columns  = indexSku, index = indexSku)\n",
    "    return df_docSimilar\n",
    "\n",
    "def getVec(model, doctags):\n",
    "    doc2vec = []\n",
    "    for doctag in doctags:\n",
    "        vec_index = [model.docvecs[tag][0] for tag in doctag]\n",
    "        doc_vecs = model.docvec_syn0[vec_index]\n",
    "        vec = np.average(doc_vecs, axis=0)\n",
    "        doc2vec.append(vec)\n",
    "    return doc2vec\n",
    "\n",
    "def evaluation(docs,model,targetSku = '124269'):\n",
    "    # generate resultcompare.csv\n",
    "    result = []\n",
    "    target_vector = model.docvecs[targetSku]\n",
    "    for vector in model.docvecs:\n",
    "        distance = spatial.distance.cosine(target_vector, vector)\n",
    "        result.append(distance)\n",
    "    docs['distance_to_1st'] = result\n",
    "    docs.sort_values(by='distance_to_1st', inplace=True)\n",
    "    docs.to_csv('resultcompare.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ =='__main__':\n",
    "    # data and model file path\n",
    "    modelpath = '../data_prepare/nlp_model/doc2vec.bin'\n",
    "    data_path = '../data_prepare/caas_stage/product.csv.gz'\n",
    "    # #load Doc2vec nlp_model from pre-trained source\n",
    "    data = pd.read_csv(data_path, compression='gzip', index_col='productSku',encoding='utf-8')\n",
    "    corpus,docs = descriptionPreprocess(data)\n",
    "    doctag = docs['productCategoryIds'].values\n",
    "    \n",
    "    train_corpus = TaggedSentence(corpus,doctag)\n",
    "    \n",
    "    #读取预训练的model\n",
    "    model = g.Doc2Vec.load(modelpath)\n",
    "    model.docvecs = DocvecsArray()\n",
    "    tfdocVec = tfDoc2vec(other_model=model, negative=5, window=1, batch_words=200)\n",
    "    \n",
    "    #训练开始\n",
    "    build_vocab(tfdocVec,train_corpus,min_count=1)\n",
    "    tfdocVec.train(train_corpus, epochs=1)\n",
    "\n",
    "    doc2vec = getVec(tfdocVec, doctag)\n",
    "    drawDiagram(doc2vec, docs, label='productName', bounds_x=None, bounds_y=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
